{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "3a3bb8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required packages\n",
    "import pandas as pd\n",
    "import json, time, urllib.parse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39153544",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac23dbc",
   "metadata": {},
   "source": [
    "## Step 1: Getting the Article, Population and Region Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ec2a7",
   "metadata": {},
   "source": [
    "### Getting Article Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deeef01",
   "metadata": {},
   "source": [
    "Here we load the csv with the relevant articles for this assignment and save the article names as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "10543a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv('/Users/zach/Jupyter/DATA 512/us_cities_by_state_SEPT.2023.csv')\n",
    "# Note that duplicates are dropped based on the article name\n",
    "articles_df = articles_df.drop_duplicates(subset=['page_title'])\n",
    "articles = articles_df['page_title'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa969ed2",
   "metadata": {},
   "source": [
    "Below we define the constants for the API pull to get the article information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "689d5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<zprice12@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = articles\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f559ebd6",
   "metadata": {},
   "source": [
    "This defines the function for requesting page information from the API for an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "cb52dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4c3ca",
   "metadata": {},
   "source": [
    "Below we take each article of interest, get the page information from the API, and then save that article and the revision ID in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba0d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame(columns=['article', 'revid'])\n",
    "for art in ARTICLE_TITLES:\n",
    "    info = request_pageinfo_per_article(art)\n",
    "    num = str(list(info['query']['pages'].keys())[0])\n",
    "    revidAdd = info['query']['pages'][num].get('lastrevid')\n",
    "    info_df.loc[len(info_df.index)] = [art, revidAdd]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87caa02e",
   "metadata": {},
   "source": [
    "### Getting State Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f8efe",
   "metadata": {},
   "source": [
    "Here we read in the state population data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "54de3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = pd.read_csv('/Users/zach/Jupyter/DATA 512/state_pops.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba867c6",
   "metadata": {},
   "source": [
    "### Getting Region Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61ea01",
   "metadata": {},
   "source": [
    "Here we read in the region data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "bb7fdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df = pd.read_csv('/Users/zach/Jupyter/DATA 512/us_regions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b1b39",
   "metadata": {},
   "source": [
    "## Step 2: Getting Article Quality Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb2046",
   "metadata": {},
   "source": [
    "Below we define the constants for the API call to get an ORES score for an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2327b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<zprice12@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "ARTICLE_REVISIONS = dict(zip(info_df.article, info_df.revid))\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6165363",
   "metadata": {},
   "source": [
    "Here we save the access token for the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "95c7a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2JlNGY4NzFmZDViNzU1Njc1NmI2ZTY1ZWQ1MWMwMyIsImp0aSI6IjdkZDVlZmZkNzdiZjZlZmQ2YmE5M2UyNmFkYmNmN2EwNjczYzI4Y2Q4ZGQyMDkyYTlhZmU2YjhkYTJmY2UyMDMxNDU3ODY0MzI3ZjE1MjdkIiwiaWF0IjoxNjk3NDMzMDA5LjgyNzIxOCwibmJmIjoxNjk3NDMzMDA5LjgyNzIyMiwiZXhwIjozMzI1NDM0MTgwOS44MjU5NzQsInN1YiI6Ijc0MDE0Mzg5IiwiaXNzIjoiaHR0cHM6Ly9tZXRhLndpa2ltZWRpYS5vcmciLCJyYXRlbGltaXQiOnsicmVxdWVzdHNfcGVyX3VuaXQiOjUwMDAsInVuaXQiOiJIT1VSIn0sInNjb3BlcyI6WyJiYXNpYyJdfQ.EE1Xieff9g50pThxSrNZErIir5yqIy7bB1Dghd7esgINFKnLqePLYw6ZOeiQW9cA9yyojVrij-6XdwrvODssZzNbLAe-HGj8CVBSqZHuG2FWlxT0fEkl4GkuDP510CzDeWq2HRBAFNghes37N4H9N4I6c9V80CnMvVIEvPueUUPP6yxMMwsVCOyabJNedZhfGXYbQ4ZGKLpENIm3xu_JM5YF5mVW5pXdhx355w1NF41eRt0OKKI10XZCbrHKMDZz7qMkQvHf8dc1mJuoE0JCNSKVCSxSZgiu0WYr6J8Urey9gfUdEazASbkFL7bYrV0ZDBW12xSdg0ynWkB-z_jrFkTncrvGG-iXuJIbSS_h7Rnq2oCe4HNaPodeGVSRKIptkirQWf0mxqsvhIaVTjtkeK-qPu-NrTjDToNbeYyD2HrWxtPHtAA0Oiy17_0foDo1lJmIDyGOZRUlZLVd-e0jNeHFTKhR80tC4pdJkBb87wSTbDhTBagSTLSZ0qgbSa3Elj-ndfBzHyf-TCl_KgeHTy-2Air1m7-KwCDZXaEdtZJzZ3fgFbIh1ATIvqRNtpxumHu_YTJt2uysphPyKsr9XlUAUPPUkg-FM-k1Lt4c3iyLnZ3ZW_oXS-JyBU21Ng2V_hc9cqxOm4vQNegQERrw92LRuFxVpIpUrdowfmV0_04'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f21b9",
   "metadata": {},
   "source": [
    "Below we define the function for retrieving an ORES score for an article from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d65919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef17b1",
   "metadata": {},
   "source": [
    "Here we calculate the ORES score for each article of interest using the article title and revision ID. If an ORES score can't be calculated, we save that article in a separate dataframe for later consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_final_df = pd.DataFrame(columns=['article_title', 'revision_id', 'article_quality'])\n",
    "info_fail_df = pd.DataFrame(columns=['article_title', 'revision_id'])\n",
    "for art in list(ARTICLE_REVISIONS.keys()):\n",
    "    try:\n",
    "        score = request_ores_score_per_article(article_revid=ARTICLE_REVISIONS[art],\n",
    "                                       email_address=\"zprice12@uw.edu\",\n",
    "                                       access_token=ACCESS_TOKEN)\n",
    "        score_val = score['enwiki']['scores'][str(ARTICLE_REVISIONS[art])]['articlequality']['score']['prediction']\n",
    "        info_final_df.loc[len(info_final_df.index)] = [art, ARTICLE_REVISIONS[art], score_val]\n",
    "    except:\n",
    "        info_fail_df.loc[len(info_fail_df.index)] = [art, ARTICLE_REVISIONS[art]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71a406",
   "metadata": {},
   "source": [
    "Below we see that the info_fail_df dataframe is empty, implying we were able to get scores for all of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "14daa48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article_title, revision_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_fail_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bdc5ed",
   "metadata": {},
   "source": [
    "## Step 3: Combining the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc8b0b6",
   "metadata": {},
   "source": [
    "Here we create a new column for state of each article using the oringal csv with article names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "72b88dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_final_df['state'] = articles_df['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07b4df",
   "metadata": {},
   "source": [
    "Below we merge the article info with the region data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "3f39b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df = info_final_df.merge(regions_df, left_on=['state'], right_on=['STATE'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd12c54",
   "metadata": {},
   "source": [
    "Below we merge the article info with the population data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "81102eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df = all_info_df.merge(pop_df, on=['state'], how='left')\n",
    "# Reorder columns\n",
    "all_info_df = all_info_df[['state','DIVISION', 'population', 'article_title', 'revision_id', 'article_quality']]\n",
    "# Rename region column\n",
    "all_info_df = all_info_df.rename(columns={'DIVISION': 'regional_division'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6840ea",
   "metadata": {},
   "source": [
    "Here we're finding the list of states that did not have an article present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "74193e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nebraska',\n",
       " 'New Hampshire',\n",
       " 'District of Columbia',\n",
       " 'North Dakota',\n",
       " 'South Dakota',\n",
       " 'Puerto Rico',\n",
       " 'North Carolina',\n",
       " 'New Jersey',\n",
       " 'Connecticut',\n",
       " 'New York',\n",
       " 'New Mexico',\n",
       " 'Rhode Island',\n",
       " 'Georgia',\n",
       " 'South Carolina',\n",
       " 'West Virginia']"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_used = all_info_df.state.unique().tolist()\n",
    "states = pop_df.state.tolist()\n",
    "list(set(states) - set(states_used))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c01e39",
   "metadata": {},
   "source": [
    "Notice that any state with a space does not have match. This is because our data used underscore instead of a space, we fix this wiht the code below. Also notice that non-states, such as District of Columbia and Puerto Rico do not have matches. Georgia has a strange spelling and also needs a mapping. Lastly, Nebraska and Connecticut simply don't have matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "043e3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_map = {'Georgia_(U.S._state)':'Georgia', 'New_Hampshire':'New Hampshire', 'North_Dakota':'North Dakota', \n",
    "            'South_Dakota':'South Dakota', 'North_Carolina':'North Carolina', 'New_Jersey':'New Jersey', \n",
    "            'New_York':'New York', 'New_Mexico':'New Mexico', 'South_Carolina': 'South Carolina', \n",
    "             'West_Virginia':'West Virginia', 'Rhode_Island':'Rhode Island'}\n",
    "all_info_df['state'] = all_info_df['state'].replace(state_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ccadab",
   "metadata": {},
   "source": [
    "Below we convert population to float type for later calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "6502e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df['population'] = all_info_df['population'].str.replace(',','').astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc35cc0",
   "metadata": {},
   "source": [
    "Here we drop duplicates of our final dataset as a final cleaning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "4783e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df = all_info_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbcec37",
   "metadata": {},
   "source": [
    "Here we save the article information file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "1d898fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df.to_csv(r'/Users/zach/Jupyter/DATA 512/wp_scored_city_articles_by_state.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b569f",
   "metadata": {},
   "source": [
    "## Step 4: Analysis / Step 5: Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd78d17",
   "metadata": {},
   "source": [
    "### 1. Top 10 US States By Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3637a63",
   "metadata": {},
   "source": [
    "Below we get the count of articles by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "10436e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df = all_info_df.groupby(['state'], as_index=False)['revision_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5304b5b",
   "metadata": {},
   "source": [
    "Here we merge the population data for each state to its article count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "2fddc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df = grp_df.merge(pop_df, on=['state'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe16b737",
   "metadata": {},
   "source": [
    "Here we calculate articles per capita in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "e7d325e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df['art_cap'] = grp_df['revision_id'].astype('float')/grp_df['population'].str.replace(',','').astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5708f6f",
   "metadata": {},
   "source": [
    "This displays the top 10 US states by coverage per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "624c2f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>art_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Maine</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>0.000342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            state   art_cap\n",
       "42        Vermont  0.000508\n",
       "31   North Dakota  0.000457\n",
       "17          Maine  0.000349\n",
       "38   South Dakota  0.000342\n",
       "13           Iowa  0.000326\n",
       "1          Alaska  0.000203\n",
       "35   Pennsylvania  0.000197\n",
       "20       Michigan  0.000177\n",
       "47        Wyoming  0.000170\n",
       "26  New Hampshire  0.000168"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_df.drop(columns=['revision_id','population'], axis=1).sort_values(by=['art_cap'], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d814b4",
   "metadata": {},
   "source": [
    "### 2. Bottom 10 US States By Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c109bd",
   "metadata": {},
   "source": [
    "This displays the bottom 10 US states by coverage per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "7e99f93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>art_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Florida</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             state   art_cap\n",
       "30  North Carolina  0.000005\n",
       "25          Nevada  0.000006\n",
       "4       California  0.000012\n",
       "2          Arizona  0.000012\n",
       "43        Virginia  0.000015\n",
       "7          Florida  0.000019\n",
       "33        Oklahoma  0.000019\n",
       "14          Kansas  0.000021\n",
       "18        Maryland  0.000025\n",
       "46       Wisconsin  0.000032"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_df.drop(columns=['revision_id','population'], axis=1).sort_values(by=['art_cap'], ascending=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd0481",
   "metadata": {},
   "source": [
    "### 3. Top 10 US States By High Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427940a",
   "metadata": {},
   "source": [
    "Here we calculate the number of featured articles and good articles per state and keep the group dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "204e78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_df = all_info_df[all_info_df['article_quality']=='GA']\n",
    "grp_df = ga_df.groupby(['state'], as_index=False)['revision_id'].count()\n",
    "fa_df = all_info_df[all_info_df['article_quality']=='FA']\n",
    "grp_df2 = fa_df.groupby(['state'], as_index=False)['revision_id'].count()\n",
    "grp_df = grp_df.merge(grp_df2, on=['state'], how='left')\n",
    "grp_df['revision_id_y'] = grp_df['revision_id_y'].fillna(value=0.0)\n",
    "grp_df['revision_id'] = grp_df['revision_id_x'] + grp_df['revision_id_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a045978",
   "metadata": {},
   "source": [
    "Here we merge population data with the grouped dataframe and calculate articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "57bfddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df = grp_df.merge(pop_df, on=['state'], how='left')\n",
    "grp_df['art_cap'] = grp_df['revision_id'].astype('float')/grp_df['population'].str.replace(',','').astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8a538",
   "metadata": {},
   "source": [
    "Below we display the top 10 US States by high quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "7c0dd044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>art_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Montana</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            state   art_cap\n",
       "42        Vermont  0.000070\n",
       "47        Wyoming  0.000067\n",
       "38   South Dakota  0.000062\n",
       "45  West Virginia  0.000060\n",
       "24        Montana  0.000049\n",
       "26  New Hampshire  0.000045\n",
       "35   Pennsylvania  0.000044\n",
       "23       Missouri  0.000043\n",
       "1          Alaska  0.000042\n",
       "27     New Jersey  0.000041"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_df.drop(columns=['revision_id','population', 'revision_id_x', 'revision_id_y',], axis=1).sort_values(by=['art_cap'], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d28002f",
   "metadata": {},
   "source": [
    "### 4. Bottom 10 US States By High Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a42af",
   "metadata": {},
   "source": [
    "Below we display the bottom 10 US States by high quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "69bf8184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>art_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Florida</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New York</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             state   art_cap\n",
       "30  North Carolina  0.000002\n",
       "43        Virginia  0.000002\n",
       "25          Nevada  0.000003\n",
       "2          Arizona  0.000003\n",
       "4       California  0.000004\n",
       "7          Florida  0.000005\n",
       "29        New York  0.000006\n",
       "18        Maryland  0.000007\n",
       "14          Kansas  0.000007\n",
       "33        Oklahoma  0.000008"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_df.drop(columns=['revision_id','population', 'revision_id_x', 'revision_id_y',], axis=1).sort_values(by=['art_cap'], ascending=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3f8f3",
   "metadata": {},
   "source": [
    "### 5. Census Divisions By Total Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76677f9",
   "metadata": {},
   "source": [
    "Below we merge the regional data with state populations to get a grouped dataframe that has the population for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "e7c31c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_pops = regions_df.merge(pop_df, left_on=['STATE'], right_on=['state']).drop(columns=['STATE'], axis=1)\n",
    "region_pops['population'] = region_pops['population'].str.replace(',','').astype('float')\n",
    "region_pops = region_pops.groupby(['DIVISION'], as_index=False)['population'].sum().rename(columns={'DIVISION': 'regional_division'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15459923",
   "metadata": {},
   "source": [
    "Here we calculate the articles for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "6025b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df = all_info_df.groupby(['regional_division'], as_index=False)['revision_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975d898",
   "metadata": {},
   "source": [
    "Below we merge the region populations with the article counts for each region and calculate articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "1299c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df = grp_df.merge(region_pops, on=['regional_division'], how='left')\n",
    "grp_df['art_cap'] = grp_df['revision_id'].astype('float')/grp_df['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e7397",
   "metadata": {},
   "source": [
    "Here we display the regions in descending order by articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "609b859f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>art_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New England</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division   art_cap\n",
       "7  West North Central  0.000134\n",
       "0  East North Central  0.000101\n",
       "1  East South Central  0.000078\n",
       "4         New England  0.000077\n",
       "2     Middle Atlantic  0.000061\n",
       "8  West South Central  0.000050\n",
       "3            Mountain  0.000042\n",
       "5             Pacific  0.000024\n",
       "6      South Atlantic  0.000011"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_df.drop(columns=['revision_id','population'], axis=1).sort_values(by=['art_cap'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6bc46",
   "metadata": {},
   "source": [
    "### 6. Census Divisions By High Quality Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847be8d7",
   "metadata": {},
   "source": [
    "Here we calculate the number of featured articles and good articles per region and keep the grouped dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "a27e35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_df = all_info_df[all_info_df['article_quality']=='GA']\n",
    "grp_df = ga_df.groupby(['regional_division'], as_index=False)['revision_id'].count()\n",
    "fa_df = all_info_df[all_info_df['article_quality']=='FA']\n",
    "grp_df2 = fa_df.groupby(['regional_division'], as_index=False)['revision_id'].count()\n",
    "grp_df = grp_df.merge(grp_df2, on=['regional_division'], how='left')\n",
    "grp_df['revision_id_y'] = grp_df['revision_id_y'].fillna(value=0.0)\n",
    "grp_df['revision_id'] = grp_df['revision_id_x'] + grp_df['revision_id_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da715367",
   "metadata": {},
   "source": [
    "Below we merge the regional data with state populations to get a grouped dataframe that has the population for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b61206e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_pops = regions_df.merge(pop_df, left_on=['STATE'], right_on=['state']).drop(columns=['STATE'], axis=1)\n",
    "region_pops['population'] = region_pops['population'].str.replace(',','').astype('float')\n",
    "region_pops = region_pops.groupby(['DIVISION'], as_index=False)['population'].sum().rename(columns={'DIVISION': 'regional_division'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66a209",
   "metadata": {},
   "source": [
    "Below we merge the region populations with the article counts for each region and calculate high quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "147779b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df = grp_df.merge(region_pops, on=['regional_division'], how='left')\n",
    "grp_df['art_cap'] = grp_df['revision_id'].astype('float')/grp_df['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c18ff",
   "metadata": {},
   "source": [
    "Here we display the regions in descending order of high quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "892844c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>art_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New England</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division   art_cap\n",
       "7  West North Central  0.000026\n",
       "1  East South Central  0.000016\n",
       "8  West South Central  0.000015\n",
       "0  East North Central  0.000015\n",
       "2     Middle Atlantic  0.000014\n",
       "3            Mountain  0.000012\n",
       "4         New England  0.000010\n",
       "5             Pacific  0.000009\n",
       "6      South Atlantic  0.000003"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_df.drop(columns=['revision_id','population', 'revision_id_x', 'revision_id_y'], axis=1).sort_values(by=['art_cap'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
